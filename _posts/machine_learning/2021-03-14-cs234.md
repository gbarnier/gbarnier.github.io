---                                                                             
layout: media                                                                   
title: "My Notes on Reinforcement Learning"
categories: ml_projects                                                            
excerpt: "These are my notes from Stanford's CS 234 and David Silver's lecture series"
permalink: cs234
ads: false                                                                      
modified: " "                                                                   
share: false                                                                    
image:                                                                          
  teaser: rl-logo.png
---
<style>
.paragraph{
    text-align: justify;
    font-size: 18px;
    width: 800px;
    color: black;
    font-family: Georgia;

}  
</style>

<h2>Reinforcement Learning</h2>
<h3>The back story</h3>
<p class="paragraph">
In the Spring 2020, I enrolled in Stanford's CS 229 class called Machine Learning and immediately became passionate about one of its subdisciplines called <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">reinforcement learning</a> (RL). RL is a family of algorithms designed to solve one of the fundamental challenges in artificial intelligence (AI) and machine learning (ML), which is <b>learning to make good decisions under uncertainty.</b><br/><br/>

In November 2020, I decided to fully commit to RL, and began learning and absorbing everything I could possibly find on the subject. I began by watching and studying the amazing David Silver's lecture series available for free on his <a href="https://www.davidsilver.uk/teaching/"> website</a> and on <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">youtube</a>. Then, I took Stanford's CS 234, a 3-month course offered in the Winter quarter and taught by Professor <a href="https://cs.stanford.edu/people/ebrun/">Emma Brunksill</a> from the Computer Science (CS) department at Stanford. CS 234 was probably the best class I have seen given at Stanford in terms of teaching, content, and help provided by TAs.

<h3>Goal of these notes</h3>
<p class="paragraph">
My goal here is not just to summarize the theorems and properties used in RL settings: there is already a lot of really good material available online for free. Here, I am trying derive the proofs of the main theorems/results, which are usually claimed to be "obvious", overlooked, or not explained in details.<br/><br/>

By going through these proofs, I managed to become comfortable manipulating the mathematical tools and learned a lot of the recurring tricks used in RL. As always, these tricks are not difficult but getting familiar with them require practice. And of course, nobody tells you explicitly how to use them, either because they don't exactly know, or because they've done it so many times that they forgot how difficult it can be at first.
<!-- Just as an example, most of the tricks involve comfortably manipulating conditional expecations and using the tower property. -->
</p>

<h3>Notes outline</h3>
<p class="paragraph">
<b>Disclaimer</b>: I am currently trying to find time to clean-up my notes, this is still a work in progress :)
</p>
<p class="paragraph">
    <ol type = "1">
        <li class="paragraph"> <a href="/papers/mdp_new.pdf" style="font-weight: bold">Markov Decision Processes</a></li>
    </ol>
</p>

<h3>Useful links</h3>
<p class="paragraph">
Here is a list of links/material I found extremely useful when I studied reinforcement learning:
</p>

<h3>Acknowledgements</h3>
<p class="paragraph">
<ul>
<li class="paragraph"> The Markov Decision Process chapter is based on the notes from my colleague <a href="https://web.stanford.edu/~rsarkar/">Rahul Sarkar</a>, former CS 234 teaching assistant (TA). Rahul gave me the inspiration for creating my own notes and adding the proofs/details that I needed to fully understand the concepts.</li>
<li class="paragraph">Steve MacCabe, one of my team mate for the CS 234 class, suggested that I put my RL notes online.</li>
</ul>
</p>
